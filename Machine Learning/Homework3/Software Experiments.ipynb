{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H3, Perceptron and NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. load\n",
    "dt = datasets.load_digits()\n",
    "X, y = dt.data, dt.target\n",
    "\n",
    "# from sklearn.datasets import fetch_openml\n",
    "# mnist = fetch_openml('mnist_784')\n",
    "\n",
    "# partition data with train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, stratify=y, random_state=37)\n",
    "\n",
    "\n",
    "print(\"train size:\", len(X_train), \"\\ntest size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. learn classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "predictor = LogisticRegression(max_iter=2000)\n",
    "predictor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = predictor.predict(X_test)\n",
    "print(\"accuracy on testing set:\", round(metrics.accuracy_score(y_test, y_pred), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden_layer_sizes : This parameter allows us to set the number of layers and the number of nodes we wish to have in the Neural Network Classifier. Each element in the tuple represents the number of nodes at the ith position where i is the index of the tuple. Thus the length of tuple denotes the total number of hidden layers in the network.\n",
    "\n",
    "max_iter: It denotes the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. learn classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "predictor = MLPClassifier(\n",
    "    hidden_layer_sizes=(10, 4), random_state=37, activation=\"logistic\", solver=\"sgd\", max_iter=2000\n",
    ")\n",
    "# predictor = MLPClassifier(random_state=42)\n",
    "predictor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = predictor.predict(X_test)\n",
    "print(\"accuracy on testing set:\", round(metrics.accuracy_score(y_test, y_pred), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss curve plots the training  error (y-axis) over the training set size (x-axis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predictor.loss_curve_)\n",
    "plt.title(\"Loss Curve\", fontsize=14)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
